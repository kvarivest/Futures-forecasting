{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[ 3  4  6  9 13 18 42 72 80 90 99]\n",
      "Index(['US Dollar Index_Ratio', 'Eurodollar_Ratio', 'AUDUSD_Ratio',\n",
      "       'Gold_Ratio', 'Soybean Oil CBOT_Ratio', 'Cotton_Ratio',\n",
      "       'Corn CBOT_stoch_ii', 'Eurodollar_Ratio_stoch_ii',\n",
      "       'SP500_Ratio_stoch_ii', 'Soybean Oil CBOT_Ratio_stoch_ii',\n",
      "       'Cotton_Ratio_stoch_i'],\n",
      "      dtype='object')\n",
      "my_cu 11\n",
      "my_pca_i 0.85\n",
      "X,shape (260, 11)\n",
      "Just dim 4\n",
      "4\n",
      "(247, 11)\n",
      "4\n",
      "Accuracy:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.62 Precision: 0.62 Recall: 0.62 f1: 0.62\n",
      "[[3 4]\n",
      " [1 5]]\n",
      "my_accuracy: 0.49 my_precision: 0.49 my_recall: 0.49 my_f1: 0.49\n",
      "[[ 0 27]\n",
      " [ 0 26]]\n",
      "4\n",
      "(247, 11)\n",
      "4\n",
      "Accuracy: 0.77 Precision: 0.77 Recall: 0.77 f1: 0.77\n",
      "[[4 3]\n",
      " [0 6]]\n",
      "my_accuracy: 0.62 my_precision: 0.62 my_recall: 0.62 my_f1: 0.62\n",
      "[[ 7 20]\n",
      " [ 0 26]]\n",
      "4\n",
      "(247, 11)\n",
      "4\n",
      "Accuracy: 0.62 Precision: 0.62 Recall: 0.62 f1: 0.62\n",
      "[[4 3]\n",
      " [2 4]]\n",
      "my_accuracy: 0.49 my_precision: 0.49 my_recall: 0.49 my_f1: 0.49\n",
      "[[ 0 27]\n",
      " [ 0 26]]\n",
      "4\n",
      "(247, 11)\n",
      "4\n",
      "Accuracy: 0.69 Precision: 0.69 Recall: 0.69 f1: 0.69\n",
      "[[4 3]\n",
      " [1 5]]\n",
      "my_accuracy: 0.49 my_precision: 0.49 my_recall: 0.49 my_f1: 0.49\n",
      "[[ 0 27]\n",
      " [ 0 26]]\n",
      "4\n",
      "(247, 11)\n",
      "4\n",
      "Accuracy: 0.62 Precision: 0.62 Recall: 0.62 f1: 0.62\n",
      "[[3 4]\n",
      " [1 5]]\n",
      "my_accuracy: 0.51 my_precision: 0.51 my_recall: 0.51 my_f1: 0.51\n",
      "[[27  0]\n",
      " [26  0]]\n",
      "4\n",
      "my_accuracy: 0.62 my_precision: 0.62 my_recall: 0.62 my_f1: 0.62\n",
      "[[3 4]\n",
      " [1 5]]\n",
      "my_accuracy: 0.53 my_precision: 0.53 my_recall: 0.53 my_f1: 0.53\n",
      "[[27  0]\n",
      " [25  1]]\n",
      "{'alpha': 0.1, 'loss': 'log', 'max_iter': 2000, 'penalty': 'l1'}\n",
      "4\n",
      "Accuracy: 0.54 Precision: 0.54 Recall: 0.54 f1: 0.54\n",
      "[[2 5]\n",
      " [1 5]]\n",
      "{'n_neighbors': 5}\n",
      "Accuracy: 0.54 Precision: 0.45 Recall: 0.45 f1: 0.45\n",
      "[[ 2 25]\n",
      " [ 4 22]]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\nesterenko\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62 Precision: 0.62 Recall: 0.62 f1: 0.62\n",
      "[[3 4]\n",
      " [1 5]]\n",
      "{'C': 0.0031622776601683794, 'penalty': 'l2'}\n",
      "Accuracy: 0.55 Precision: 0.55 Recall: 0.55 f1: 0.55\n",
      "[[27  0]\n",
      " [24  2]]\n",
      "{'C': 0.0031622776601683794, 'penalty': 'l2'}\n",
      "4\n",
      "Accuracy: 0.69 Precision: 0.69 Recall: 0.69 f1: 0.69\n",
      "[[3 4]\n",
      " [0 6]]\n",
      "{'kernel': 'rbf'}\n",
      "Accuracy: 0.51 Precision: 0.51 Recall: 0.51 f1: 0.51\n",
      "[[ 2 25]\n",
      " [ 1 25]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d3544989b98d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    863\u001b[0m                 \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m                 \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best: %f using %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 408\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[1;34m(input_iterator)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[1;32m---> 73\u001b[1;33m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[0;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[1;32m--> 760\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2132\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m               training=training))\n\u001b[0m\u001b[0;32m    253\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reduction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m           \u001b[0mper_sample_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m           weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[0;32m    168\u001b[0m               \u001b[0mper_sample_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[0;32m    220\u001b[0m           y_pred, y_true)\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m   return K.sparse_categorical_crossentropy(\n\u001b[1;32m--> 978\u001b[1;33m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   4539\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mupdate_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4540\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4541\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4543\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_is_symbolic_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10392\u001b[0m                         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10393\u001b[0m                         \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10394\u001b[1;33m                         shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[0;32m  10395\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10396\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[0;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    794\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3429\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3431\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1773\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1774\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1605\u001b[0m     \u001b[1;31m# TODO(skyewm): this creates and deletes a new TF_Status for every attr.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m     \u001b[1;31m# It might be worth creating a convenient way to re-use the same status.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1607\u001b[1;33m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "import math\n",
    "import os, sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "display_settings = {\n",
    "    'max_columns': 10000,\n",
    "    'expand_frame_repr': True,  #    \n",
    "    'max_rows': 100,\n",
    "    'precision': 2,\n",
    "    'show_dimensions': True\n",
    "}\n",
    "\n",
    "for op, value in display_settings.items():\n",
    "    pd.set_option(\"display.{}\".format(op), value)\n",
    "\n",
    "# os.chdir(r'C:\\Users\\nesterenko\\OneDrive - API & Interstarch\\Desktop\\NN\\max_1')\n",
    "\n",
    "# col_name_string = 'Ethanol CBOT, USD/gallon'\n",
    "# col_name_string = 'Raw Sugar 11'\n",
    "# col_name_string = 'Wheat MATIF, USD/MT'\n",
    "# col_name_string = 'Wheat CBOT'\n",
    "col_name_string = 'Corn CBOT'\n",
    "# col_name_string = 'US Dollar Index'   #USDBRL\n",
    "# col_name_string = 'USDBRL'   #USDBRL\n",
    "# col_name_string = 'Crude Oil WTI'\n",
    "# col_name_string = 'Soybeans CBOT'\n",
    "# col_name_string = 'Cotton'\n",
    "# col_name_string = 'Coffee'\n",
    "# col_name_string = 'Cococa'\n",
    "# col_name_string = 'Live Cattle'\n",
    "\n",
    "\n",
    "my_period = [12]\n",
    "my_method = [1,2,3,4,5,7]  #1,2,3,4,5,7\n",
    "my_cutted = [0.1, 0.25, 0.5, 0.75, 0.9]  # 0.1, 0.25, 0.5, 0.75, 0.9\n",
    "my_pca = [ 0.85, 0.95, 0.97]  # 0.85, 0.95, 0.97\n",
    "\n",
    "for my_method_i in my_method:\n",
    "    for my_per_i in my_period:\n",
    "        for my_cutted_i in my_cutted:\n",
    "            for my_pca_i in my_pca:\n",
    "                def conv(df):\n",
    "                    \n",
    "                    def add_ratio(i):\n",
    "                        a2 = i + '_Ratio'    \n",
    "                        df[a2] = df[col_name_string]/df[i] \n",
    "                    def add_change(i):\n",
    "                        a1 = i + '_ch'\n",
    "                        df[a1] = df[i].pct_change()*100 \n",
    "                    \n",
    "                    def add_change(i):\n",
    "                        a1 = i + '_ch'\n",
    "                        df[a1] = df[i].pct_change(periods = 1)*100    \n",
    "                    def add_change_2(i):\n",
    "                        a1 = i + '_ch_i'\n",
    "                        df[a1] = df[i].pct_change(periods = 2)*100 \n",
    "                    def add_change_3(i):\n",
    "                        a1 = i + '_ch_i'\n",
    "                        df[a1] = df[i].pct_change(periods = 2)*100              \n",
    "                    def add_stoch_20(i):\n",
    "                        a2 = i + '_stoch'    \n",
    "                        df[a2] = 2* ((df[i] - df[i].rolling(20).min().shift(1)) / (df[i].rolling(20).max().shift(1) - df[i].rolling(20).min().shift(1)) - 0.5)                                \n",
    "                    def add_stoch_90(i):\n",
    "                        a2 = i + '_stoch'    \n",
    "                        df[a2] = 2* ((df[i] - df[i].rolling(90).min().shift(1)) / (df[i].rolling(90).max().shift(1) - df[i].rolling(90).min().shift(1)) - 0.5)                                \n",
    "                    def add_stoch_190(i):\n",
    "                        a2 = i + '_stoch_i'    \n",
    "                        df[a2] = 2* ((df[i] - df[i].rolling(190).min().shift(1)) / (df[i].rolling(190).max().shift(1) - df[i].rolling(190).min().shift(1)) - 0.5)                                \n",
    "                    def add_stoch_250(i):\n",
    "                        a2 = i + '_stoch_ii'    \n",
    "                        df[a2] = 2* ((df[i] - df[i].rolling(250).min().shift(1)) / (df[i].rolling(250).max().shift(1) - df[i].rolling(250).min().shift(1)) - 0.5)                                \n",
    "                    def add_mean_two(i):  \n",
    "                        a2 = i+ \"_mean\"\n",
    "                        a3 = i+ \"_mean_i\"   \n",
    "                        df[a2] = df[i].rolling(250).mean().shift(1)\n",
    "                        df[a3] = df[i].rolling(190).mean().shift(1)\n",
    "                    def add_mean_90(i):    \n",
    "                        a4 = i+ \"_mean_iii\"\n",
    "                        df[a4] = df[i].rolling(90).mean().shift(1)\n",
    "                    def add_mean_one(i):  \n",
    "                        a2 = i+ \"__mean\"\n",
    "                        a3 = i+ \"__mean_i\"   \n",
    "                        a4 = i+ \"__mean_ii\"\n",
    "                        df[a2] = df[i].rolling(5).mean().shift(1)\n",
    "                        df[a3] = df[i].rolling(10).mean().shift(6)\n",
    "                        df[a4] = df[i].rolling(20).mean().shift(11)   \n",
    "                    def add_mean_ch_two(i):  \n",
    "                        a2 = i+ \"_mean_ch_\"\n",
    "                        a3 = i+ \"_mean_ch__i\"   \n",
    "                        df[a2] = df[i]/df[i].rolling(250).mean().shift(1)\n",
    "                        df[a3] = df[i]/df[i].rolling(190).mean().shift(1)\n",
    "                    def add_mean_ch_90(i):    \n",
    "                        a4 = i+ \"_mean_ch__iii\"\n",
    "                        df[a4] = df[i]/df[i].rolling(90).mean().shift(1)                        \n",
    "                    def add_mean_ch_one(i):  \n",
    "                        a2 = i+ \"__mean_ch_\"\n",
    "                        a3 = i+ \"__mean_ch__i\"   \n",
    "                        a4 = i+ \"__mean_ch__ii\"\n",
    "                        df[a2] = df[i]/df[i].rolling(5).mean().shift(1)\n",
    "                        df[a3] = df[i]/df[i].rolling(10).mean().shift(6)\n",
    "                        df[a4] = df[i]/df[i].rolling(20).mean().shift(11)  \n",
    "\n",
    "                    def split_by_cat(row):\n",
    "                        if row > 0:\n",
    "                            return 1\n",
    "                        else:\n",
    "                            return 0\n",
    "                    df.set_index('Date', inplace = True)\n",
    "\n",
    "#                     if cut_method_i == 0:\n",
    "                    c = df.columns\n",
    "                    for i in c:\n",
    "                        if i != col_name_string:\n",
    "                            add_ratio(i)\n",
    "                        else:\n",
    "                            add_change(i)\n",
    "                        \n",
    "                    if my_method_i == 1:\n",
    "                        c = df.columns\n",
    "                        for i in c:  \n",
    "                            add_stoch_190(i)\n",
    "                            add_stoch_250(i)\n",
    "                    if my_method_i == 2:\n",
    "                        c = df.columns\n",
    "                        for i in c:  \n",
    "                            add_mean_ch_two(i)\n",
    "                    if my_method_i == 3:\n",
    "                        c = df.columns\n",
    "                        for i in c:\n",
    "                            add_stoch_190(i)\n",
    "                            add_stoch_250(i)\n",
    "                            add_mean_ch_two(i)\n",
    "                    if my_method_i == 4: \n",
    "                        c = df.columns\n",
    "                        for i in c:\n",
    "                            add_stoch_90(i)\n",
    "                            add_stoch_190(i)\n",
    "                            add_stoch_250(i)\n",
    "                            add_mean_ch_90(i)\n",
    "                            add_mean_ch_two(i)  \n",
    "                    if my_method_i == 5:\n",
    "                        c = df.columns\n",
    "                        for i in c:  \n",
    "                            add_stoch_20(i)\n",
    "                            add_stoch_90(i)\n",
    "                            add_stoch_190(i)\n",
    "                            add_stoch_250(i)\n",
    "                            add_mean_ch_one(i)\n",
    "                            add_mean_ch_two(i) \n",
    "                            add_mean_ch_90(i) \n",
    "                    if my_method_i == 6:\n",
    "                        c = df.columns\n",
    "                        for i in c:  \n",
    "                            add_mean_one(i)\n",
    "                            add_mean_two(i)\n",
    "                            \n",
    "                    if my_method_i == 7:\n",
    "                        c = df.columns\n",
    "                        for i in c:  \n",
    "                            add_change(i)\n",
    "                            add_change_2(i)\n",
    "                            add_change_3(i)   \n",
    "                        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "                        df.fillna( method ='ffill', inplace = True)\n",
    "                        c = df.columns\n",
    "                        for i in c: \n",
    "                            add_stoch_190(i)\n",
    "                            add_stoch_250(i)\n",
    "                            add_mean_ch_two(i)\n",
    "\n",
    "\n",
    "                    df['prelabel'] = df[col_name_string].rolling(my_per_i).mean().shift(-my_per_i)\n",
    "                    df['label'] = df['prelabel'] / df[col_name_string] * 100.0 - 100.0\n",
    "                    df.drop(columns =['prelabel'], inplace = True) \n",
    "\n",
    "                    df['cat'] = df['label'].apply(lambda row: split_by_cat(row))\n",
    "                    df['label'] = df['cat'] \n",
    "                    df.drop(columns =['cat'], inplace = True) \n",
    "                    df = df.drop(list(df)[0:cn], axis=1)  \n",
    "                    return df\n",
    "                df = pd.read_excel(open('weekly_av_prices.xlsx', 'rb'), skiprows= 0,  header = [0], parse_dates = True )\n",
    "                cvc = df.columns[1:]\n",
    "                cn = cvc.shape[0]\n",
    "                df = conv(df)\n",
    "                df_fin = df.copy()\n",
    "                \n",
    "                df_fin = df_fin.replace([np.inf, -np.inf], np.nan)\n",
    "                df_fin.fillna( method ='ffill', inplace = True) \n",
    "\n",
    "\n",
    "\n",
    "                my_sh = df_fin.shape[1]\n",
    "                my_cu = int(round(my_sh*my_cutted_i,0))\n",
    "\n",
    "                print(my_cu)\n",
    "\n",
    "\n",
    "                from sklearn.feature_selection import SelectKBest\n",
    "                from sklearn.feature_selection import f_classif\n",
    "\n",
    "                df = df_fin.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                selector = SelectKBest(f_classif, k= my_cu)\n",
    "                selector.fit(X, y)\n",
    "                cols = selector.get_support(indices=True)\n",
    "                print(cols)\n",
    "                df_cutted = df_fin.iloc[:, cols]\n",
    "                print(df_cutted.columns)\n",
    "                df_cutted[\"label\"] = df_fin[\"label\"].copy()\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()        \n",
    "                X = np.array(df.drop(['label'], 1))        \n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                X = scale_X.transform(X)        \n",
    "\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_pca_i)\n",
    "                pca.fit(X)\n",
    "                my_dim  =  pca.n_components_\n",
    "\n",
    "                print(\"my_cu\",my_cu)\n",
    "                print(\"my_pca_i\", my_pca_i)\n",
    "                print(\"X,shape\", X.shape)       \n",
    "                print(\"Just dim\", my_dim)\n",
    "\n",
    "\n",
    "                ## GPC BOOST\n",
    "\n",
    "                from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "                from sklearn.gaussian_process.kernels import RBF\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                cv = StratifiedShuffleSplit(n_splits=3,  random_state=42)\n",
    "\n",
    "\n",
    "                kernel = 1.0 * RBF(1.0)\n",
    "                clf_GPC = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                print(my_dim)\n",
    "                print(X_train.shape)\n",
    "                #1\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test   \n",
    "#                 y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "#                 y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "                clf_GPC.fit(X_train, y_train_bin)\n",
    "                p = clf_GPC.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = y\n",
    "                X = pca.transform(X)\n",
    "                # clf_RF.fit(X, y_bin)\n",
    "                p = clf_GPC.predict(X)\n",
    "                gpc_acc = accuracy_score(y_bin, p)\n",
    "                my_precision = precision_score(y_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(gpc_acc, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "                \n",
    "                ## GB\n",
    "\n",
    "                from sklearn.ensemble import GradientBoostingClassifier\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                cv = StratifiedShuffleSplit(n_splits=3,  random_state=42)\n",
    "          \n",
    "                clf_GB = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                print(my_dim)\n",
    "                print(X_train.shape)\n",
    "                #1\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test   \n",
    "#                 y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "#                 y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "                clf_GB.fit(X_train, y_train_bin)\n",
    "                p = clf_GB.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = y\n",
    "                X = pca.transform(X)\n",
    "                # clf_RF.fit(X, y_bin)\n",
    "                p = clf_GB.predict(X)\n",
    "                GB_acc = accuracy_score(y_bin, p)\n",
    "                my_precision = precision_score(y_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(GB_acc, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)                \n",
    "                \n",
    "                ## NS\n",
    "\n",
    "                from sklearn.svm import NuSVC\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                cv = StratifiedShuffleSplit(n_splits=3,  random_state=42)\n",
    "          \n",
    "                clf_NS = NuSVC(probability=True)\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                print(my_dim)\n",
    "                print(X_train.shape)\n",
    "                #1\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test   \n",
    "#                 y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "#                 y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "                clf_NS.fit(X_train, y_train_bin)\n",
    "                p = clf_NS.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = y\n",
    "                X = pca.transform(X)\n",
    "                # clf_RF.fit(X, y_bin)\n",
    "                p = clf_NS.predict(X)\n",
    "                NS_acc = accuracy_score(y_bin, p)\n",
    "                my_precision = precision_score(y_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(NS_acc, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)                                \n",
    "                \n",
    "                ## LDA\n",
    "\n",
    "                from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                cv = StratifiedShuffleSplit(n_splits=3,  random_state=42)\n",
    "          \n",
    "                clf_LDA = LDA()\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                print(my_dim)\n",
    "                print(X_train.shape)\n",
    "                #1\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test   \n",
    "#                 y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "#                 y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "                clf_LDA.fit(X_train, y_train_bin)\n",
    "                p = clf_LDA.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = y\n",
    "                X = pca.transform(X)\n",
    "                # clf_RF.fit(X, y_bin)\n",
    "                p = clf_LDA.predict(X)\n",
    "                LDA_acc = accuracy_score(y_bin, p)\n",
    "                my_precision = precision_score(y_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(LDA_acc, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)                \n",
    "                \n",
    "                \n",
    "                ## QDA\n",
    "\n",
    "                from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                cv = StratifiedShuffleSplit(n_splits=3,  random_state=42)\n",
    "          \n",
    "                clf_QD = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                print(my_dim)\n",
    "                print(X_train.shape)\n",
    "                #1\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test   \n",
    "#                 y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "#                 y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "                clf_QD.fit(X_train, y_train_bin)\n",
    "                p = clf_QD.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = y\n",
    "                X = pca.transform(X)\n",
    "                # clf_RF.fit(X, y_bin)\n",
    "                p = clf_QD.predict(X)\n",
    "                QD_acc = accuracy_score(y_bin, p)\n",
    "                my_precision = precision_score(y_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_bin, p)  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(QD_acc, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                ## Stochastic Gradient Desscent\n",
    "\n",
    "                from sklearn.linear_model import SGDClassifier\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "                # df_cutted = df_cuttedby_rf.copy()\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                X = scale_X.fit_transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                2#\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "\n",
    "\n",
    "\n",
    "                params = {\n",
    "                    \"max_iter\" : [100, 500, 1000, 2000],\n",
    "                    \"loss\" : [\"log\",  \"modified_huber\"],   #\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"\n",
    "                    \"alpha\" : [0.0001, 0.001, 0.01, 0.1, 0.15],\n",
    "                    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "                }\n",
    "\n",
    "                model = SGDClassifier() #max_iter=1000\n",
    "                ovo_clf = GridSearchCV(model, param_grid=params)\n",
    "\n",
    "\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test\n",
    "\n",
    "                ovo_clf.fit(X_train, y_train_bin)\n",
    "                predictions = ovo_clf.predict(X_test)\n",
    "\n",
    "                my_accuracy = accuracy_score(y_test_bin, predictions)\n",
    "                my_precision = precision_score(y_test_bin, predictions, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, predictions, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, predictions, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, predictions)   #.argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(my_accuracy, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "                # df = df_fin.loc['2019'].copy()\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = y\n",
    "                X = pca.transform(X)\n",
    "                p = ovo_clf.predict(X)\n",
    "\n",
    "                sgd_acc = accuracy_score(y_bin, p)\n",
    "                my_precision = precision_score(y_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_bin, p)   #.argmax(axis=1)\n",
    "                print(\"my_accuracy:\", round(sgd_acc, 2),\"my_precision:\", round(my_precision,2), \"my_recall:\", round(my_recall,2) , \"my_f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "                print(ovo_clf.best_params_)\n",
    "                # pr = ovo_clf.predict_proba(X)\n",
    "                # pr\n",
    "\n",
    "                ## KNeighbors\n",
    "\n",
    "                from sklearn.neighbors import KNeighborsClassifier\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit \n",
    "                from sklearn.inspection import permutation_importance\n",
    "                # cv = StratifiedShuffleSplit(n_splits=3,  random_state=42)\n",
    "\n",
    "                #     param_grid = [{ 'max_depth' : [ 3, 6]}]\n",
    "                #     forest_reg = DecisionTreeClassifier(random_state=42)\n",
    "                #     model = GridSearchCV(forest_reg, param_grid, cv=3 , scoring= 'roc_auc' )\n",
    "\n",
    "                #     param_grid = [{ 'n_estimators' : [ 3 , 10, 200 ], 'max_features': [ 2 , 4 , 6 , 8 ]}, {'bootstrap':[False], 'n_estimators' : [ 3 , 10 ], 'max_features' : [ 2 , 3 , 4 ]}]\n",
    "\n",
    "\n",
    "                k_range = list(range(1, 35))\n",
    "                param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "                # param_grid = { 'n_estimators': [500], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth' : [7], 'criterion' :['gini', 'entropy']}\n",
    "                forest_reg = KNeighborsClassifier()\n",
    "                kn_RF = GridSearchCV(forest_reg, param_grid, cv=3 , scoring= 'accuracy' ) \n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                X = scale_X.fit_transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                3#\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "                # y_train_bin = y_train\n",
    "                # y_test_bin = y_test   \n",
    "                y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "                y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "                kn_RF.fit(X_train, y_train_bin)\n",
    "                p = kn_RF.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin.argmax(axis=1), p.argmax(axis=1))  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "                print(kn_RF.best_params_)\n",
    "\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                y =  tf.keras.utils.to_categorical(y)\n",
    "                X = pca.transform(X)\n",
    "                p = kn_RF.predict(X)\n",
    "                kn_acc = accuracy_score(y, p)\n",
    "                my_precision = precision_score(y, p, average='micro')\n",
    "                my_recall = recall_score(y, p, average='micro')\n",
    "                my_f1 = f1_score(y, p, average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y.argmax(axis=1), p.argmax(axis=1))  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "\n",
    "                ## Logistics Regression\n",
    "\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                grid={\"C\":np.logspace(-4,-2,25), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "                logreg=LogisticRegression()\n",
    "                logreg_cv=GridSearchCV(logreg,grid, cv=3)\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                X = scale_X.fit_transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                4#\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test  \n",
    "\n",
    "                logreg_cv.fit(X_train, y_train)\n",
    "                p = logreg_cv.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p , average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p )  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "                print(logreg_cv.best_params_)\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X = pca.transform(X) \n",
    "                p = logreg_cv.predict(X)\n",
    "                lg_acc = accuracy_score(y, p)\n",
    "                my_precision = precision_score(y, p, average='micro')\n",
    "                my_recall = recall_score(y, p, average='micro')\n",
    "                my_f1 = f1_score(y, p , average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y, p )  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(lg_acc, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "                print(logreg_cv.best_params_)\n",
    "\n",
    "                ## SVC\n",
    "\n",
    "                from sklearn.svm import SVC\n",
    "                from sklearn.model_selection import GridSearchCV  \n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "                # param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "                #               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                #               'kernel': ['rbf' ,\"poly\"]} \n",
    "\n",
    "                param_grid = { 'kernel': ['rbf' ,\"poly\"]} \n",
    "                svc_clf=SVC(probability=True)\n",
    "                svc_cv=GridSearchCV(svc_clf, param_grid, cv=3)\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                X = scale_X.fit_transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                5#\n",
    "                pca.fit(X_train)\n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "                print(pca.n_components_)\n",
    "\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test  \n",
    "\n",
    "                svc_cv.fit(X_train, y_train)\n",
    "                p = svc_cv.predict(X_test)\n",
    "                my_accuracy = accuracy_score(y_test_bin, p)\n",
    "                my_precision = precision_score(y_test_bin, p, average='micro')\n",
    "                my_recall = recall_score(y_test_bin, p, average='micro')\n",
    "                my_f1 = f1_score(y_test_bin, p , average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y_test_bin, p )  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(my_accuracy, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "                print(svc_cv.best_params_)\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                X = pca.transform(X) \n",
    "                p = svc_cv.predict(X)\n",
    "                svc_acc = accuracy_score(y, p)\n",
    "                my_precision = precision_score(y, p, average='micro')\n",
    "                my_recall = recall_score(y, p, average='micro')\n",
    "                my_f1 = f1_score(y, p , average='micro') # , average='micro'\n",
    "                my_cm = confusion_matrix(y, p )  #.argmax(axis=1) .argmax(axis=1)\n",
    "                print(\"Accuracy:\", round(svc_acc, 2),\"Precision:\", round(my_precision,2), \"Recall:\", round(my_recall,2) , \"f1:\", round(my_f1,2))\n",
    "                print(my_cm)\n",
    "\n",
    "\n",
    "                # Neural Networks 2014-2018\n",
    "\n",
    "                from tensorflow import keras\n",
    "                from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                # df = df_cutted.loc['2014':'2018']\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                X = scale_X.transform(X)\n",
    "                y = np.array(df['label'])\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                6#\n",
    "                pca.fit(X_train) \n",
    "                X_train = pca.transform(X_train) \n",
    "                X_test = pca.transform(X_test) \n",
    "\n",
    "                y_train_bin = y_train\n",
    "                y_test_bin = y_test\n",
    "\n",
    "        #         y_train_bin = tf.keras.utils.to_categorical(y_train)\n",
    "        #         y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "                a = X_train.shape[1]\n",
    "\n",
    "                def create_model():\n",
    "                    model = tf.keras.Sequential()\n",
    "                    model._estimator_type = \"classifier\"\n",
    "                    model.add(layers.Dense(a, input_dim=a, activation='relu'))\n",
    "                #     model.add(layers.Dense(a/2, input_dim=a, activation='relu'))\n",
    "                    model.add(layers.Dense(2, activation='softmax'))\n",
    "                    model.compile(loss='sparse_categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "                    return model\n",
    "\n",
    "                Kmodel = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "                Kmodel._estimator_type = \"classifier\"\n",
    "\n",
    "                batch_size = [50, 300, 500]\n",
    "                epochs = [50, 100, 300]\n",
    "                param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "                grid = GridSearchCV(estimator=Kmodel, param_grid=param_grid, cv=3)\n",
    "                nn = grid.fit(X_train, y_train_bin)\n",
    "\n",
    "                print(\"Best: %f using %s\" % (nn.best_score_, nn.best_params_))\n",
    "                means = nn.cv_results_['mean_test_score']\n",
    "                stds = nn.cv_results_['std_test_score']\n",
    "                params = nn.cv_results_['params']\n",
    "                for mean, stdev, param in zip(means, stds, params):\n",
    "                    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "                ## NN 2019\n",
    "\n",
    "                batch_size_i = nn.best_params_[\"batch_size\"]\n",
    "                epochs_i = nn.best_params_[\"epochs\"]\n",
    "                model = create_model()\n",
    "                model.fit(X_train, y_train_bin, epochs=epochs_i, batch_size=batch_size_i)\n",
    "        #         history = model.fit(X_train, y_train_bin, epochs=epochs_i, batch_size=batch_size_i)\n",
    "        #         plt.plot(history.history['accuracy'])\n",
    "        #         plt.ylabel('accuracy')\n",
    "        #         plt.xlabel('epoch')\n",
    "        #         plt.show()\n",
    "\n",
    "                scores = model.evaluate(X_test, y_test_bin)\n",
    "                first_score = round(scores[1]*100 , 2)\n",
    "                print(first_score)\n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                # df = df_cutted.loc['2019']\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                y = np.array(df['label'])\n",
    "                y_bin = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "                # s=[]\n",
    "                # n = len(y_bin)\n",
    "                # for i in range(n):\n",
    "                #     s.append(y_test_bin[i][1])\n",
    "                # y_bin = s \n",
    "\n",
    "\n",
    "                # y =  tf.keras.utils.to_categorical(y)\n",
    "                X = scale_X.transform(X)\n",
    "                X = pca.transform(X)\n",
    "                print(X.shape)\n",
    "                p = model.predict_classes(X)\n",
    "\n",
    "                my_cm = confusion_matrix(y, p) \n",
    "                print(my_cm)\n",
    "                # scores = model.evaluate(p, y_bin)\n",
    "                # first_score = round(scores[1]*100 , 2)\n",
    "                # print(first_score)\n",
    "\n",
    "                # s = []\n",
    "                # l = len(y)\n",
    "                # for i in range(0, l):\n",
    "                #     s.append(i)\n",
    "                # print(s)\n",
    "                # print(len(s))\n",
    "                # print(len(y))\n",
    "                # plt.plot( p , \"g-\", linestyle = 'dotted', linewidth = 6, color = 'darkblue' ) \n",
    "                # plt.scatter( s, y , color='black') \n",
    "                # plt.scatter( s, p, color='red') \n",
    "\n",
    "                nn_acc = accuracy_score(y, p)\n",
    "                print(nn_acc)\n",
    "                # print(y)\n",
    "                # print(p)\n",
    "                # print(my_cm[0,1])\n",
    "\n",
    "                ## ensemble         \n",
    "                \n",
    "                from sklearn.ensemble import VotingClassifier \n",
    "                voting_clf = VotingClassifier(\n",
    "                    estimators= [ (\"SGD_ovo\", ovo_clf), (\"KN\", kn_RF), ( 'lr', logreg_cv), ('svc', svc_cv), (\"nn\", nn), ( 'GPC', clf_GPC), ( 'QD', clf_QD),  ( 'GB', clf_GB), ( 'LDA', clf_LDA),  ( 'NS', clf_NS)], \n",
    "                    voting= 'hard' )\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                X = scale_X.transform(X)\n",
    "                print(X.shape)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                pca.fit(X)\n",
    "                X = pca.transform(X) \n",
    "                print(X.shape)\n",
    "\n",
    "                voting_clf.fit(X, y) \n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                X = pca.transform(X) \n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                p = voting_clf.predict(X)\n",
    "                \n",
    "                \n",
    "                my_cm = confusion_matrix(y, p) \n",
    "                print(my_cm)\n",
    "                hard_acc = accuracy_score(y, p)\n",
    "                print(hard_acc)\n",
    "\n",
    "\n",
    "                from sklearn.ensemble import VotingClassifier \n",
    "                voting_clf = VotingClassifier(\n",
    "                    estimators= [ (\"SGD_ovo\", ovo_clf), (\"KN\", kn_RF), ( 'lr', logreg_cv), ('svc', svc_cv), (\"nn\", nn), ( 'GPC', clf_GPC), ( 'QD', clf_QD),  ( 'GB', clf_GB), ( 'LDA', clf_LDA),  ( 'NS', clf_NS)], \n",
    "                    voting= 'soft' )\n",
    "\n",
    "                df = df_cutted.loc['2014':'2018'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                scale_X = StandardScaler()\n",
    "                scale_X.fit(X)\n",
    "                X = scale_X.transform(X)\n",
    "                print(X.shape)\n",
    "                y = np.array(df['label'])\n",
    "\n",
    "                from sklearn.decomposition import PCA\n",
    "                pca = PCA(n_components = my_dim)\n",
    "                pca.fit(X)\n",
    "                X = pca.transform(X) \n",
    "                print(X.shape)\n",
    "\n",
    "                voting_clf.fit(X, y) \n",
    "\n",
    "                df = df_cutted.loc['2019'].copy()\n",
    "                X = np.array(df.drop(['label'], 1))\n",
    "                X = scale_X.transform(X)\n",
    "                X = pca.transform(X) \n",
    "                y = np.array(df['label'])\n",
    "\n",
    "\n",
    "#                 print(X)\n",
    "                p = voting_clf.predict(X)\n",
    "                my_cm = confusion_matrix(y, p) \n",
    "                print(my_cm)\n",
    "                soft_acc = accuracy_score(y, p)\n",
    "                print(soft_acc)                \n",
    "                pr = voting_clf.predict_proba(X)\n",
    "                \n",
    "                my_threshold = 0.65\n",
    "                my_hthreshold = 0.75\n",
    "                pp=[]\n",
    "                ppp = []\n",
    "                yy = []\n",
    "                yyy = []\n",
    "                s = []\n",
    "                ss = []\n",
    "                n = len(y)\n",
    "                for i in range(n):\n",
    "                    a = pr[i][0]\n",
    "                    b = pr[i][1]\n",
    "                    z = max(a,b)\n",
    "                    s.append(z) \n",
    "                    if z > my_threshold:\n",
    "                            pp.append(p[i])\n",
    "                            yy.append(y[i])\n",
    "                    if z > my_hthreshold:\n",
    "                            ppp.append(p[i])\n",
    "                            yyy.append(y[i])                            \n",
    "                soft_acc_th = accuracy_score(yy, pp)\n",
    "                print(\"Soft_acc_th\", soft_acc_th)\n",
    "                soft_acc_hth = accuracy_score(yyy, ppp)\n",
    "                print(\"Soft_acc_hth\", soft_acc_hth)\n",
    "                l = len(pp)\n",
    "                ll=len(ppp)\n",
    "                f = open('ensemble_register.txt', 'a')\n",
    "                f.write(\"MARKET\" + \"^\" + str(col_name_string) + \"^\" +\n",
    "                        \"my_per_i\" + \"^\" + str(my_per_i)  + \"^\" +\n",
    "                        \"my_method_i\" + \"^\" + str(my_method_i)  + \"^\" +\n",
    "                        \"my_cutted_i\" + \"^\" + str(my_cutted_i)  + \"^\" +\n",
    "                        \"my_pca_i\"  + \"^\" + str(my_pca_i) + \"^\" + \n",
    "                        \"gpc_acc\"  + \"^\" + str(gpc_acc) + \"^\" +\n",
    "                        \"QD_acc\"  + \"^\" + str(QD_acc) + \"^\" +\n",
    "                        \"GB_acc\"  + \"^\" + str(GB_acc) + \"^\" +\n",
    "                        \"LDA_acc\"  + \"^\" + str(LDA_acc) + \"^\" +\n",
    "                        \"NS_acc\"  + \"^\" + str(NS_acc) + \"^\" +\n",
    "                        \"sgd_acc\"  + \"^\" + str(sgd_acc) + \"^\" +\n",
    "                        \"kn_acc\"  + \"^\" + str(kn_acc) + \"^\" +\n",
    "                        \"lg_acc\"  + \"^\" + str(lg_acc) + \"^\" +\n",
    "                        \"svc_acc\"  + \"^\" + str(svc_acc) + \"^\" +\n",
    "                        \"nn_acc\"  + \"^\" + str(nn_acc) + \"^\" +        \n",
    "                        \"hard_acc\" + \"^\" + str(hard_acc) + \"^\" +\n",
    "                        \"soft_acc\" + \"^\" + str(soft_acc)  + \"^\" +\n",
    "                        \"soft_acc_th\" + \"^\" + str(soft_acc_th) + \"^\" +\n",
    "                        \"soft_acc_hth\" + \"^\" + str(soft_acc_hth) + \"^\" +\n",
    "                        \"l\" + \"^\" + str(l)  + \"^\" +\n",
    "                        \"ll\" + \"^\" + str(ll)  + \"^\" +  \"\\n\")\n",
    "                f.close                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                print(\"Average proba\", np.sum(s)/len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
